{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('assignment4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('wholesale-customers-data.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Channel: integer (nullable = true)\n",
      " |-- Region: integer (nullable = true)\n",
      " |-- Fresh: integer (nullable = true)\n",
      " |-- Milk: integer (nullable = true)\n",
      " |-- Grocery: integer (nullable = true)\n",
      " |-- Frozen: integer (nullable = true)\n",
      " |-- Detergents_Paper: integer (nullable = true)\n",
      " |-- Delicassen: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+------------------+------------------+------------------+\n",
      "|Region|Channel|Count|   Avg(Delicassen)|         Avg(Milk)|      Avg(Grocery)|\n",
      "+------+-------+-----+------------------+------------------+------------------+\n",
      "|     3|      1|  211|1518.2843601895734|3486.9810426540284| 3886.734597156398|\n",
      "|     3|      2|  105|1826.2095238095237|10981.009523809524|15953.809523809523|\n",
      "|     2|      2|   19|            1239.0|  9190.78947368421|16326.315789473685|\n",
      "|     2|      1|   28| 1105.892857142857|           2304.25|            4395.5|\n",
      "|     1|      2|   18|1871.9444444444443|           10784.0|18471.944444444445|\n",
      "|     1|      1|   59|1197.1525423728813|3870.2033898305085| 4026.135593220339|\n",
      "+------+-------+-----+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We could also use code commented below, but I could not give specific names for columns in that way\n",
    "# Thus I used pyspark.sql.functions library below \n",
    "# df.groupBy(\"Region\", \"Channel\").agg({\"Delicassen\": \"count\", \"Milk\": \"avg\", \"Grocery\": \"avg\", \"Frozen\": \"avg\"}).show()\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "df.groupBy(\"Region\", \"Channel\").agg(func.count(\"Delicassen\").alias(\"Count\"), func.mean(\"Delicassen\").alias(\"Avg(Delicassen)\"), func.mean(\"Milk\").alias(\"Avg(Milk)\"), func.mean(\"Grocery\").alias(\"Avg(Grocery)\")).orderBy(func.desc(\"Region\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "ml_df = df[\"Fresh\", \"Detergents_Paper\", \"Delicassen\"]\n",
    "feat_cols = [\"Fresh\", \"Detergents_Paper\", \"Delicassen\"]\n",
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
    "vec_data = vec_assembler.transform(ml_df)\n",
    "\n",
    "# normalize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"norm_features\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(vec_data)\n",
    "norm_fin_data = scalerModel.transform(vec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Fresh: int, Detergents_Paper: int, Delicassen: int, features: vector, norm_features: vector]\n",
      "For k=2:\n",
      "Cost is 1041.2845273708995\n",
      "Prediction distribution table: \n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|    1|\n",
      "|         0|  439|\n",
      "+----------+-----+\n",
      "\n",
      "For k=3:\n",
      "Cost is 761.2088587153988\n",
      "Prediction distribution table: \n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|    1|\n",
      "|         2|   70|\n",
      "|         0|  369|\n",
      "+----------+-----+\n",
      "\n",
      "For k=4:\n",
      "Cost is 509.2171027652562\n",
      "Prediction distribution table: \n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|    1|\n",
      "|         3|   35|\n",
      "|         2|   64|\n",
      "|         0|  340|\n",
      "+----------+-----+\n",
      "\n",
      "For k=5:\n",
      "Cost is 465.39255314808526\n",
      "Prediction distribution table: \n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|    1|\n",
      "|         3|   76|\n",
      "|         4|  327|\n",
      "|         2|    1|\n",
      "|         0|   35|\n",
      "+----------+-----+\n",
      "\n",
      "+-----+----------------+----------+--------------------+--------------------+----------+\n",
      "|Fresh|Detergents_Paper|Delicassen|            features|       norm_features|prediction|\n",
      "+-----+----------------+----------+--------------------+--------------------+----------+\n",
      "|12669|            2674|      1338|[12669.0,2674.0,1...|[1.00171349501181...|         4|\n",
      "| 7057|            3293|      1776|[7057.0,3293.0,17...|[0.55798343470663...|         4|\n",
      "| 6353|            3516|      7844|[6353.0,3516.0,78...|[0.50231950697056...|         4|\n",
      "|13265|             507|      1788|[13265.0,507.0,17...|[1.04883807019747...|         4|\n",
      "|22615|            1777|      5185|[22615.0,1777.0,5...|[1.78812461044220...|         3|\n",
      "| 9413|            1795|      1451|[9413.0,1795.0,14...|[0.74426782923247...|         4|\n",
      "|12126|            3140|       545|[12126.0,3140.0,5...|[0.95877952802220...|         4|\n",
      "| 7579|            3321|      2566|[7579.0,3321.0,25...|[0.59925697203366...|         4|\n",
      "| 5963|            1716|       750|[5963.0,1716.0,75...|[0.47148295609404...|         4|\n",
      "| 6006|            7425|      2098|[6006.0,7425.0,20...|[0.47488288349837...|         4|\n",
      "| 3366|            5977|      1744|[3366.0,5977.0,17...|[0.26614315448810...|         4|\n",
      "|13146|             549|       497|[13146.0,549.0,49...|[1.03942896877617...|         4|\n",
      "|31714|            3881|      2931|[31714.0,3881.0,2...|[2.50756506281512...|         3|\n",
      "|21217|            6707|       602|[21217.0,6707.0,6...|[1.67758743576175...|         4|\n",
      "|24653|            5058|      2168|[24653.0,5058.0,2...|[1.94926535579180...|         3|\n",
      "|10253|             964|       412|[10253.0,964.0,41...|[0.81068501573574...|         4|\n",
      "| 1020|            4508|      1080|[1020.0,4508.0,10...|[0.08064944075397...|         4|\n",
      "| 5876|             370|      4478|[5876.0,370.0,447...|[0.46460403320620...|         4|\n",
      "|18601|            2767|      3181|[18601.0,2767.0,3...|[1.47074534065157...|         3|\n",
      "| 7780|            2518|       501|[7780.0,2518.0,50...|[0.61514965594695...|         4|\n",
      "+-----+----------------+----------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[Row(Fresh=36847, Detergents_Paper=239, Delicassen=47943, features=DenseVector([36847.0, 239.0, 47943.0]), norm_features=DenseVector([2.9134, 0.0501, 17.0004]), prediction=1)]\n"
     ]
    }
   ],
   "source": [
    "print(norm_fin_data)\n",
    "for i in range(2,6):\n",
    "    print(\"For k=\" + str(i) + \":\")\n",
    "    kmeans = KMeans(featuresCol='norm_features').setK(i).setSeed(2459501)\n",
    "    model4 = kmeans.fit(norm_fin_data)\n",
    "    wssse_4 = model4.computeCost(norm_fin_data)\n",
    "    print(\"Cost is \" + str(wssse_4))\n",
    "    print(\"Prediction distribution table: \")\n",
    "    model4.transform(norm_fin_data).groupBy('prediction').count().show()\n",
    "\n",
    "model4.transform(norm_fin_data).show()\n",
    "#just to check values of prediction 1 \n",
    "print(model4.transform(norm_fin_data).where(\"prediction == 1\").collect())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
